{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erike-Simon/CESAR-AED/blob/main/ProcDados_structured_streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7frF0tWOCNZq",
        "outputId": "46619859-f303-465b-b883-37def59861ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZZMGrD8CtS_",
        "outputId": "3dcabf60-0423-4f7c-8544-1e13b42e6fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=ca1ac9902e8309452586c2118aa67e1dc6afb2b3bf44488b4de9ea5d271e88df\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Window"
      ],
      "metadata": {
        "id": "qunK8SOBCuTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando um cluster local com 2 workers, 1 cores por worker e 3GB de RAM por worker\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "    .master(\"local-cluster[2, 1, 3072]\")\\\n",
        "    .getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "iSDh69SuDGAz",
        "outputId": "5cbfc6a1-bf3c-43d6-ad3e-19460b2a2eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7cf0bcf2e800>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://26375e278a45:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local-cluster[2, 1, 3072]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/proc-dados-larga-escala/data\""
      ],
      "metadata": {
        "id": "dpSKzLmODTKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parar spark session\n",
        "# spark.stop()"
      ],
      "metadata": {
        "id": "LlXopyubD0zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo de Structured Streaming usando Complete Mode"
      ],
      "metadata": {
        "id": "U6yW-2lXD8Pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estrutura básica do Streaming"
      ],
      "metadata": {
        "id": "SoCjrEdKD_TI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines = spark.readStream\\\n",
        "    .format('socket')\\\n",
        "    .option('host', 'localhost')\\\n",
        "    .option('port', 10000)\\\n",
        "    .load()\n",
        "print(type(lines))\n",
        "\n",
        "lineCounts = lines.groupBy('value')\\\n",
        "    .count()\n",
        "print(type(lineCounts))\n",
        "\n",
        "def foreach_batch_function(df, epoch_id):\n",
        "    print(epoch_id)\n",
        "    df.show()\n",
        "\n",
        "query = lineCounts.writeStream\\\n",
        "    .outputMode('complete')\\\n",
        "    .foreachBatch(foreach_batch_function)\\\n",
        "    .start()\n",
        "print(type(query))"
      ],
      "metadata": {
        "id": "vNoXldRGD5ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2e2f2ad-1a5d-4170-c8dd-300c1fe8173a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n",
            "<class 'pyspark.sql.dataframe.DataFrame'>\n",
            "<class 'pyspark.sql.streaming.query.StreamingQuery'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "yyfxAA9zG2--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo word count com Streaming"
      ],
      "metadata": {
        "id": "ljslex76JXdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines = spark.readStream\\\n",
        "    .format('socket')\\\n",
        "    .option('host', 'localhost')\\\n",
        "    .option('port', 10000)\\\n",
        "    .load()\n",
        "\n",
        "words = lines.select(\n",
        "    F.explode(\n",
        "        F.split(lines['value'], ' ')\n",
        "    ).alias('word')\n",
        ")\n",
        "\n",
        "wordCounts = words.groupBy('word').count()\n",
        "\n",
        "def foreach_batch_function(df, epoch_id):\n",
        "    print(epoch_id)\n",
        "    df.show()\n",
        "\n",
        "query = wordCounts.writeStream\\\n",
        "    .outputMode('complete')\\\n",
        "    .foreachBatch(foreach_batch_function)\\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "k4dRuUCEJXxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "R2fcKX1SJ7xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo de Structured Streaming usando a coluna Timestamp"
      ],
      "metadata": {
        "id": "SruuHYW4OhbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines = spark.readStream\\\n",
        "    .format('socket')\\\n",
        "    .option('host', 'localhost')\\\n",
        "    .option('port', 10000)\\\n",
        "    .option('includeTimestamp', 'true')\\\n",
        "    .load()\n",
        "\n",
        "words = lines.select(\n",
        "    'timestamp',\n",
        "    F.explode(\n",
        "        F.split(lines['value'], ' ')\n",
        "    ).alias('word')\n",
        ")\n",
        "\n",
        "wordCounts = words.groupBy('timestamp', 'word').count()\n",
        "\n",
        "def foreach_batch_function(df, epoch_id):\n",
        "    print(epoch_id)\n",
        "    df.show()\n",
        "\n",
        "query = wordCounts.writeStream\\\n",
        "    .outputMode('complete')\\\n",
        "    .foreachBatch(foreach_batch_function)\\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "7oZ5iiZkOh6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "EVZ_ljyyP0uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo de Structured Streaming usando Window"
      ],
      "metadata": {
        "id": "INIe6BPXT60o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines = spark.readStream\\\n",
        "    .format('socket')\\\n",
        "    .option('host', 'localhost')\\\n",
        "    .option('port', 10000)\\\n",
        "    .option('includeTimestamp', 'true')\\\n",
        "    .load()\n",
        "\n",
        "words = lines.select(\n",
        "    'timestamp',\n",
        "    F.explode(\n",
        "        F.split(lines['value'], ' ')\n",
        "    ).alias('word')\n",
        ")\n",
        "\n",
        "wordCounts = words\\\n",
        "    .groupBy(\n",
        "        F.window('timestamp', '10 seconds', '10 seconds'),\n",
        "        'word'\n",
        "    ).count()\\\n",
        "    .orderBy(F.asc('window.start'), F.asc('count'))\n",
        "\n",
        "def foreach_batch_function(df, epoch_id):\n",
        "    print(epoch_id)\n",
        "    print(df.toPandas())\n",
        "\n",
        "query = wordCounts.writeStream\\\n",
        "    .outputMode('complete')\\\n",
        "    .foreachBatch(foreach_batch_function)\\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "l3e6bX4VT6Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "MifppPcXVKj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo de Structured Streaming usando Update Mode"
      ],
      "metadata": {
        "id": "MLS_jakJXLQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines = spark\\\n",
        "    .readStream\\\n",
        "    .format('socket')\\\n",
        "    .option('host', 'localhost')\\\n",
        "    .option('port', 10000)\\\n",
        "    .option('includeTimestamp', 'true')\\\n",
        "    .load()\n",
        "\n",
        "# Split the lines into words\n",
        "words = lines.select(\n",
        "    'timestamp',\n",
        "    F.explode(\n",
        "        F.split(lines['value'], ' ')\n",
        "    ).alias('word')\n",
        ")\n",
        "\n",
        "wordCounts = words\\\n",
        "    .groupBy(\n",
        "        F.window('timestamp', '10 seconds', '10 seconds'),\n",
        "        'word'\n",
        "    ).count()\n",
        "\n",
        "def foreach_batch_function(df, epoch_id):\n",
        "    print(epoch_id)\n",
        "    print(df.toPandas())\n",
        "\n",
        "query = wordCounts\\\n",
        "    .writeStream\\\n",
        "    .outputMode('update')\\\n",
        "    .foreachBatch(foreach_batch_function)\\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "N4sf0QIfXLht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "9TP94GkCXWUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo de Structured Streaming usando Append Mode"
      ],
      "metadata": {
        "id": "H1XUe-28lRQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines = spark\\\n",
        "    .readStream\\\n",
        "    .format('socket')\\\n",
        "    .option('host', 'localhost')\\\n",
        "    .option('port', 10000)\\\n",
        "    .option('includeTimestamp', 'true')\\\n",
        "    .load()\n",
        "\n",
        "# Split the lines into words\n",
        "words = lines.select(\n",
        "    'timestamp',\n",
        "    F.explode(\n",
        "        F.split(lines['value'], ' ')\n",
        "    ).alias('word')\n",
        ")\n",
        "\n",
        "wordCounts = words\\\n",
        "    .withWatermark('timestamp', '10 seconds')\\\n",
        "    .groupBy(\n",
        "        F.window('timestamp', '10 seconds', '10 seconds'),\n",
        "        'word'\n",
        "    ).count()\n",
        "\n",
        "def foreach_batch_function(df, epoch_id):\n",
        "    print(epoch_id)\n",
        "    print(df.toPandas())\n",
        "\n",
        "query = wordCounts\\\n",
        "    .writeStream\\\n",
        "    .outputMode('append')\\\n",
        "    .foreachBatch(foreach_batch_function)\\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "8ck__7xPlRmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "sA_hGq5MlUES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplos de Structured Streaming usando arquivos CSV"
      ],
      "metadata": {
        "id": "XdlG6k9X2ApK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os \"output modes\" (modos de saída) são uma característica importante do Structured Streaming no Apache Spark que define como os resultados do processamento de streaming são escritos para um destino de saída. Existem três modos de saída principais:\n",
        "\n",
        "1. **Complete Mode (Modo Completo):** Neste modo, o resultado completo de cada microbacth é escrito para o destino de saída. Isso significa que o resultado reflete o estado completo do conjunto de dados no final de cada microbacth, o que pode incluir dados duplicados se o mesmo dado tiver sido processado várias vezes devido a falhas ou reexecuções. É útil para casos em que você precisa de uma visão completa e atualizada do estado atual do stream em cada etapa.\n",
        "\n",
        "2. **Append Mode (Modo Adição):** Neste modo, apenas os novos resultados de cada microbacth são escritos para o destino de saída. Isso significa que apenas as novas linhas ou eventos que foram processados desde a última execução são adicionados ao destino de saída. Se o mesmo dado for processado novamente, ele não será incluído no resultado, evitando duplicatas. É útil quando você está apenas interessado nos novos dados adicionados ao stream e não precisa manter o estado completo.\n",
        "\n",
        "3. **Update Mode (Modo Atualização):** Este modo é semelhante ao modo completo, mas apenas as linhas que foram atualizadas desde a última execução são escritas para o destino de saída. Isso é útil quando você está interessado apenas nas linhas que foram modificadas ou atualizadas no stream. No entanto, o modo de atualização só funciona se o seu processamento de streaming incluir operações que podem ser identificadas como \"upserts\" ou atualizações no conjunto de dados.\n",
        "\n",
        "A escolha do modo de saída depende dos requisitos específicos do seu aplicativo e do tipo de análise que você está realizando. Cada modo tem suas vantagens e limitações, e é importante selecionar o modo mais apropriado com base nas necessidades do seu aplicativo e nos requisitos de saída."
      ],
      "metadata": {
        "id": "dofJMdmx58V3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exemplo com dados de bitcoin"
      ],
      "metadata": {
        "id": "97t6IFAN2MBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\n",
        "    f\"{ROOT_DATA_PATH}/bigfile.csv\",\n",
        "    schema=\"Height INTEGER, \\\n",
        "      Input STRING, \\\n",
        "      Output STRING, \\\n",
        "      Sum STRING, \\\n",
        "      Time TIMESTAMP\"\n",
        ")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff7t8pxL2Bjr",
        "outputId": "ff7a1356-4a19-4ae1-e697-c901b29a5527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Height: integer (nullable = true)\n",
            " |-- Input: string (nullable = true)\n",
            " |-- Output: string (nullable = true)\n",
            " |-- Sum: string (nullable = true)\n",
            " |-- Time: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIYKyx763RzB",
        "outputId": "ea2583a5-75e9-4535-9265-b690788988f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+--------------------+------+-------------------+\n",
            "|Height|               Input|              Output|   Sum|               Time|\n",
            "+------+--------------------+--------------------+------+-------------------+\n",
            "|   546|['1DZTzaBHUDM7T3Q...|['1KAD5EnzzLtrSo2...|['25']|2009-01-15 06:08:20|\n",
            "|   546|['1KAD5EnzzLtrSo2...|['1KAD5EnzzLtrSo2...|['25']|2009-01-15 06:08:20|\n",
            "|   546|['1KAD5EnzzLtrSo2...|['1DZTzaBHUDM7T3Q...|['25']|2009-01-15 06:08:20|\n",
            "+------+--------------------+--------------------+------+-------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "df.select(F.sum('Height')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzl4mD493rZB",
        "outputId": "c2e76617-4b69-465d-ed63-adf3618dd368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|  sum(Height)|\n",
            "+-------------+\n",
            "|2589395531434|\n",
            "+-------------+\n",
            "\n",
            "CPU times: user 309 ms, sys: 60 ms, total: 369 ms\n",
            "Wall time: 1min 4s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmkKYbO13zbc",
        "outputId": "ccf724db-514f-4cd0-a08f-54c06f1f29ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 85.3 ms, sys: 5.56 ms, total: 90.9 ms\n",
            "Wall time: 16.1 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13494203"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputStream = spark.readStream.csv(\n",
        "    f\"{ROOT_DATA_PATH}/bitcoin\",\n",
        "    schema=\"Height INTEGER, \\\n",
        "      Input STRING, \\\n",
        "      Output STRING, \\\n",
        "      Sum STRING, \\\n",
        "      Time TIMESTAMP\"\n",
        ")\n",
        "\n",
        "# inputStream = inputStream.select(F.sum('Height'))\n",
        "inputStream = inputStream.select(F.count('Height')) # Modifica o DataFrame de streaming 'inputStream' usando a função\n",
        "                                                    # 'select' para calcular a contagem de valores na coluna \"Height\".\n",
        "\n",
        "def foreach_batch_function(df, epoch_id):\n",
        "    print(epoch_id)\n",
        "    print(df.toPandas())\n",
        "\n",
        "# 1. Configura a escrita do DataFrame de streaming para um sink (destino) usando 'writeStream';\n",
        "# 2. O método 'outputMode('update')' define o modo de saída como \"update\", o que significa que\n",
        "#    apenas as linhas atualizadas desde a última execução serão escritas no sink;\n",
        "# 3. Aplica a função 'foreachBatch' para cada microbacth de dados;\n",
        "# 4. Inicia a consulta de streaming usando 'start()'. Isso inicia o processo de execução do fluxo.\n",
        "#    Durante a execução, o Spark divide os dados de entrada em microbatches e aplica as transformações\n",
        "#    definidas no inputStream a cada microbatch.\n",
        "query = inputStream\\\n",
        "    .writeStream\\\n",
        "    .outputMode('update')\\\n",
        "    .foreachBatch(foreach_batch_function)\\\n",
        "    .start()\n",
        "query.awaitTermination(480)"
      ],
      "metadata": {
        "id": "207C_qA031zy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "7d4d4696-aa63-41e8-f197-4ef003325d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-3b60b22ab7aa>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mforeachBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforeach_batch_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/streaming/query.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    217\u001b[0m                     \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_value\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 )\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "b9Wis5-040Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Qual é a quantidade total de eventos de cada anúncio nos últimos 10 segundos? Calcular a cada 10 segundos e use update mode"
      ],
      "metadata": {
        "id": "Ku_BgqibEWT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método **window** é usado para definir janelas de tempo no Spark Streaming. Ele permite agrupar os dados em intervalos específicos de tempo para análise. Aqui está uma explicação detalhada dos parâmetros do método window:\n",
        "\n",
        "* **colName:** Este é o nome da coluna que contém os timestamps dos eventos. No seu caso, é 'timestamp', indicando que a janela de tempo será baseada nos timestamps presentes nessa coluna.\n",
        "\n",
        "* **windowDuration:** Este parâmetro especifica o tamanho da janela de tempo. No seu código, é definido como '10 seconds', o que significa que cada janela de tempo terá uma duração de 10 segundos.\n",
        "\n",
        "* **slideDuration (opcional):** Este parâmetro especifica o intervalo de tempo entre o início de uma janela e a próxima. No seu código, também é definido como '10 seconds', o que significa que as janelas são deslocadas a cada 10 segundos."
      ],
      "metadata": {
        "id": "7vZSKGGJy1wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputStream = spark.readStream.csv(\n",
        "    f\"{ROOT_DATA_PATH}/ad_action_exercises\",\n",
        "    schema=\"timestamp TIMESTAMP, \\\n",
        "      user_id STRING, \\\n",
        "      action STRING, \\\n",
        "      adId STRING, \\\n",
        "      campaignId STRING\"\n",
        ")\n",
        "\n",
        "# 1. Os dados do streaming são agrupados por janelas de tempo\n",
        "#    de 10 segundos (usando a função 'window', segundo parâmetro), com sliding\n",
        "#    também de 10 segundos (terceiro parâmetro) e também pelo campo 'adId';\n",
        "# 2. Em seguida,a contagem de registros é feita para cada grupo pelo 'count()'.\n",
        "inputStream = inputStream\\\n",
        "    .groupBy(\n",
        "        F.window('timestamp', '10 seconds', '10 seconds'),\n",
        "        'adId'\n",
        "    ).count()\n",
        "\n",
        "def foreach_batch_function(df, epoch_id):\n",
        "    df = df.orderBy('window.start')\n",
        "    print(epoch_id)\n",
        "    print(df.toPandas())\n",
        "\n",
        "query = inputStream\\\n",
        "    .writeStream\\\n",
        "    .outputMode('update')\\\n",
        "    .foreachBatch(foreach_batch_function)\\\n",
        "    .start()\n",
        "query.awaitTermination(60)"
      ],
      "metadata": {
        "id": "zOa0i8-K5Lig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66a6d7de-935d-472f-f4c9-819b0cd4bc3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "                                        window     adId  count\n",
            "0   (2023-09-01 02:42:00, 2023-09-01 02:42:10)  adId_06   9840\n",
            "1   (2023-09-01 02:42:00, 2023-09-01 02:42:10)  adId_05   8681\n",
            "2   (2023-09-01 02:42:00, 2023-09-01 02:42:10)  adId_09   9212\n",
            "3   (2023-09-01 02:42:00, 2023-09-01 02:42:10)  adId_03   8241\n",
            "4   (2023-09-01 02:42:00, 2023-09-01 02:42:10)  adId_02   9187\n",
            "5   (2023-09-01 02:42:00, 2023-09-01 02:42:10)  adId_07   9507\n",
            "6   (2023-09-01 02:42:00, 2023-09-01 02:42:10)  adId_08   8164\n",
            "7   (2023-09-01 02:42:00, 2023-09-01 02:42:10)  adId_10   5374\n",
            "8   (2023-09-01 02:42:00, 2023-09-01 02:42:10)  adId_04   8840\n",
            "9   (2023-09-01 02:42:00, 2023-09-01 02:42:10)  adId_01   7864\n",
            "10  (2023-09-01 02:42:10, 2023-09-01 02:42:20)  adId_04   7975\n",
            "11  (2023-09-01 02:42:10, 2023-09-01 02:42:20)  adId_02   8281\n",
            "12  (2023-09-01 02:42:10, 2023-09-01 02:42:20)  adId_03   7535\n",
            "13  (2023-09-01 02:42:10, 2023-09-01 02:42:20)  adId_09   8186\n",
            "14  (2023-09-01 02:42:10, 2023-09-01 02:42:20)  adId_05   7763\n",
            "15  (2023-09-01 02:42:10, 2023-09-01 02:42:20)  adId_01   7217\n",
            "16  (2023-09-01 02:42:10, 2023-09-01 02:42:20)  adId_10   4837\n",
            "17  (2023-09-01 02:42:10, 2023-09-01 02:42:20)  adId_06   8682\n",
            "18  (2023-09-01 02:42:10, 2023-09-01 02:42:20)  adId_07   8289\n",
            "19  (2023-09-01 02:42:10, 2023-09-01 02:42:20)  adId_08   7648\n",
            "20  (2023-09-01 02:42:20, 2023-09-01 02:42:30)  adId_06   9731\n",
            "21  (2023-09-01 02:42:20, 2023-09-01 02:42:30)  adId_07   8990\n",
            "22  (2023-09-01 02:42:20, 2023-09-01 02:42:30)  adId_09   8758\n",
            "23  (2023-09-01 02:42:20, 2023-09-01 02:42:30)  adId_08   9096\n",
            "24  (2023-09-01 02:42:20, 2023-09-01 02:42:30)  adId_05   8479\n",
            "25  (2023-09-01 02:42:20, 2023-09-01 02:42:30)  adId_01   8337\n",
            "26  (2023-09-01 02:42:20, 2023-09-01 02:42:30)  adId_04   8525\n",
            "27  (2023-09-01 02:42:20, 2023-09-01 02:42:30)  adId_10   5140\n",
            "28  (2023-09-01 02:42:20, 2023-09-01 02:42:30)  adId_03   8993\n",
            "29  (2023-09-01 02:42:20, 2023-09-01 02:42:30)  adId_02   8851\n",
            "30  (2023-09-01 02:42:30, 2023-09-01 02:42:40)  adId_01    857\n",
            "31  (2023-09-01 02:42:30, 2023-09-01 02:42:40)  adId_09    862\n",
            "32  (2023-09-01 02:42:30, 2023-09-01 02:42:40)  adId_07    843\n",
            "33  (2023-09-01 02:42:30, 2023-09-01 02:42:40)  adId_10    545\n",
            "34  (2023-09-01 02:42:30, 2023-09-01 02:42:40)  adId_03    928\n",
            "35  (2023-09-01 02:42:30, 2023-09-01 02:42:40)  adId_08    890\n",
            "36  (2023-09-01 02:42:30, 2023-09-01 02:42:40)  adId_05    872\n",
            "37  (2023-09-01 02:42:30, 2023-09-01 02:42:40)  adId_02    858\n",
            "38  (2023-09-01 02:42:30, 2023-09-01 02:42:40)  adId_06    935\n",
            "39  (2023-09-01 02:42:30, 2023-09-01 02:42:40)  adId_04    900\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ao observar as contagens de registros para cada janela de tempo, você está vendo quantos registros foram observados em intervalos de 10 segundos para cada 'adId'. Como o Spark Streaming processa os dados em batches, a mudança na época indica que um novo batch de dados foi processado para essa janela de tempo específica, e o resultado é a contagem de registros para esse batch em particular"
      ],
      "metadata": {
        "id": "prDf0Js0Ww1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "MWOBXv3iKmtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quais são os top 3 anúncios e intervalos com mais eventos considerando todos os intervalos de janela? Calcule com uma janela de 10 segundos e periodicidade de 10 segundos e use complete mode.\n",
        "\n"
      ],
      "metadata": {
        "id": "ANGAOLzWEiqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputStream = spark.readStream.csv(\n",
        "    f\"{ROOT_DATA_PATH}/ad_action_exercises\",\n",
        "    schema=\"timestamp TIMESTAMP, \\\n",
        "      user_id STRING, \\\n",
        "      action STRING, \\\n",
        "      adId STRING, \\\n",
        "      campaignId STRING\"\n",
        ")\n",
        "\n",
        "inputStream = inputStream\\\n",
        "    .groupBy(\n",
        "        F.window('timestamp', '10 seconds', '10 seconds'),\n",
        "        'adId'\n",
        "    ).count()\\\n",
        "    .orderBy(F.desc('count'))\\\n",
        "    .limit(3)\n",
        "\n",
        "def foreach_batch_function(df, epoch_id):\n",
        "    print(epoch_id)\n",
        "    print(df.toPandas())\n",
        "\n",
        "query = inputStream\\\n",
        "    .writeStream\\\n",
        "    .outputMode('complete')\\\n",
        "    .foreachBatch(foreach_batch_function)\\\n",
        "    .start()\n",
        "query.awaitTermination(30)"
      ],
      "metadata": {
        "id": "zVXTe1m3EliB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5b7cf67-13f0-4109-dd2f-d4548c5202ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "                                       window     adId  count\n",
            "0  (2023-09-01 02:42:00, 2023-09-01 02:42:10)  adId_06   9840\n",
            "1  (2023-09-01 02:42:20, 2023-09-01 02:42:30)  adId_06   9731\n",
            "2  (2023-09-01 02:42:00, 2023-09-01 02:42:10)  adId_07   9507\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "Bzzf5XcJOckT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quais são os top 3 anúncios com mais eventos dos últimos 10 segundos em cada intervalo de janela? Calcule a cada 10 segundos e use o complete mode"
      ],
      "metadata": {
        "id": "69dletDIEn0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputStream = spark.readStream.csv(\n",
        "    f\"{ROOT_DATA_PATH}/ad_action_exercises\",\n",
        "    schema=\"timestamp TIMESTAMP, \\\n",
        "      user_id STRING, \\\n",
        "      action STRING, \\\n",
        "      adId STRING, \\\n",
        "      campaignId STRING\"\n",
        ")\n",
        "\n",
        "inputStream = inputStream\\\n",
        "    .groupBy(\n",
        "        F.window('timestamp', '10 seconds', '10 seconds'),\n",
        "        'adId'\n",
        "    ).count()\n",
        "\n",
        "def foreach_batch_function(df, epoch_id):\n",
        "    window_group = Window.partitionBy('window.start')\\\n",
        "        .orderBy(F.desc('count'))\n",
        "    df = df.withColumn('rank', F.row_number().over(window_group))\\\n",
        "        .where(F.col('rank') <= 3)\\\n",
        "        .drop('rank')\\\n",
        "        .orderBy(F.asc('window.start'))\n",
        "    print(epoch_id)\n",
        "    print(df.toPandas())\n",
        "\n",
        "query = inputStream\\\n",
        "    .writeStream\\\n",
        "    .outputMode('complete')\\\n",
        "    .foreachBatch(foreach_batch_function)\\\n",
        "    .start()\n",
        "query.awaitTermination(40)"
      ],
      "metadata": {
        "id": "hE5myYUyEoIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5df8a53-24fc-414d-f819-6d2fde6dfca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "QclAJMNMPlrC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "435c631e-08df-4aa8-a1ad-39367771f22e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
            "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/utils.py\", line 120, in call\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/utils.py\", line 117, in call\n",
            "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
            "  File \"<ipython-input-6-f2eddeb85302>\", line 24, in foreach_batch_function\n",
            "    print(df.toPandas())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/conversion.py\", line 202, in toPandas\n",
            "    rows = self.collect()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\", line 1257, in collect\n",
            "    sock_info = self._jdf.collectToPython()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
            "    raise Py4JJavaError(\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o72.collectToPython.\n",
            ": java.lang.InterruptedException\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1040)\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1345)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:975)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n",
            "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n",
            "\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)\n",
            "\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)\n",
            "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:296)\n",
            "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:179)\n",
            "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:173)\n",
            "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:195)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
            "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n",
            "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n",
            "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n",
            "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n",
            "\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:445)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4160)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n",
            "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4157)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy31.call(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}