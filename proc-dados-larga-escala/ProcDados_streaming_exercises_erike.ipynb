{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erike-Simon/CESAR-AED/blob/main/ProcDados_streaming_exercises_erike.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQL_yiZ8L-XU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46082de5-a274-4610-d183-06a0aa0e0eeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M48mJ2zvMBYK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bbf97bc-0b34-48b6-a8ed-e2c1b0ed4392"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=5d042c9d7b32a6c8e803c3c971c60ae63a8599042204f5743a99e93d30234863\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o7-P0T7L3NZ"
      },
      "source": [
        "## Dados\n",
        "\n",
        "**Descrição das colunas:**  \n",
        "timestamp,user_id,action,adId,campaignId\n",
        "\n",
        "**Amostra:**  \n",
        "2016-09-21 22:11:00,7c74953c-66cc-48bd-9d02-a02bf039cf3f,click,adId_09,campaignId_01  \n",
        "2016-06-25 18:29:00,676a083e-2f8e-4ff2-9ec2-270f7f9d6033,view,adId_09,campaignId_02  \n",
        "2016-02-14 19:03:00,77158997-0dfa-48b7-9149-973dc151ef8d,click,adId_02,campaignId_02  \n",
        "2016-03-26 06:27:00,78aa2467-b502-413b-94e9-04ec8210bd13,click,adId_07,campaignId_03\n",
        "\n",
        "**Path:**  \n",
        "data/ad_action_exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCNzcpw9L3Nb"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "import pyspark.sql.types as T\n",
        "\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuuD0XrYL3Nb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "93373878-e1a3-4e98-9c7e-a2b239b62784"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f9e50b5a800>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://afff42447bea:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local-cluster[2, 1, 3072]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Criando um cluster local com 2 workers, 1 cores por worker e 3GB de RAM por worker\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "    .master('local-cluster[2, 1, 3072]')\\\n",
        "    .getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trJhBnStNLsI"
      },
      "outputs": [],
      "source": [
        "AD_ACTION_CSV_PATH = \"drive/MyDrive/Data/ad_action_exercises\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwKE1LwaL3Nc"
      },
      "outputs": [],
      "source": [
        "# Stop spark session\n",
        "# spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVfQV1MzL3Nc"
      },
      "source": [
        "## Foi utilizado o notebook split_ad_action_data.ipynb para criar os dados do streaming escrevendo arquivos CSV na pasta `data/ad_action_exercises`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNrveklOL3Nd"
      },
      "source": [
        "## Resposta Atividade 1\n",
        "\n",
        "Qual é a quantidade total de clicks e a quantidade total views dos últimos 10 segundos? Calcular a cada 10 segundos.\n",
        "\n",
        "**Resposta:**  \n",
        "```\n",
        "                                       window action  count\n",
        "0  (2023-09-01 02:42:10, 2023-09-01 02:42:20)  click  53452\n",
        "1  (2023-09-01 02:42:30, 2023-09-01 02:42:40)   view   2544\n",
        "2  (2023-09-01 02:42:00, 2023-09-01 02:42:10)   view  25630\n",
        "3  (2023-09-01 02:42:00, 2023-09-01 02:42:10)  click  59280\n",
        "4  (2023-09-01 02:42:20, 2023-09-01 02:42:30)  click  59627\n",
        "5  (2023-09-01 02:42:10, 2023-09-01 02:42:20)   view  22961\n",
        "6  (2023-09-01 02:42:30, 2023-09-01 02:42:40)  click   5946\n",
        "7  (2023-09-01 02:42:20, 2023-09-01 02:42:30)   view  25273\n",
        "```\n",
        "Se somar o count de view dos intervalos nós devemos ter 76408 eventos\n",
        "\n",
        "Se somar o count de click dos intervalos nós devemos ter 178305 eventos\n",
        "\n",
        "A ordem dos registros pode variar. Não foi solicitada nenhuma ordenação na questão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2KUeEiYL3Nd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4762ca7-379c-43b2-e190-7ee0e5279dc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "                                       window action  count\n",
            "0  (2023-09-01 02:42:10, 2023-09-01 02:42:20)  click  53452\n",
            "1  (2023-09-01 02:42:30, 2023-09-01 02:42:40)   view   2544\n",
            "2  (2023-09-01 02:42:00, 2023-09-01 02:42:10)   view  25630\n",
            "3  (2023-09-01 02:42:00, 2023-09-01 02:42:10)  click  59280\n",
            "4  (2023-09-01 02:42:20, 2023-09-01 02:42:30)  click  59627\n",
            "5  (2023-09-01 02:42:10, 2023-09-01 02:42:20)   view  22961\n",
            "6  (2023-09-01 02:42:30, 2023-09-01 02:42:40)  click   5946\n",
            "7  (2023-09-01 02:42:20, 2023-09-01 02:42:30)   view  25273\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "inputStream = spark.readStream.csv(\n",
        "    AD_ACTION_CSV_PATH,\n",
        "    schema=\"timestamp TIMESTAMP, \\\n",
        "        user_id STRING, \\\n",
        "        action STRING, \\\n",
        "        adId STRING, \\\n",
        "        campaignId STRING\"\n",
        ")\n",
        "\n",
        "inputStream = inputStream\\\n",
        "    .groupBy(\n",
        "        F.window('timestamp', '10 seconds', '10 seconds'),\n",
        "        'action'\n",
        "    ).count()\\\n",
        "\n",
        "def foreach_batch_function(df, epoch_id):\n",
        "    print(epoch_id)\n",
        "    print(df.toPandas())\n",
        "\n",
        "query = inputStream\\\n",
        "    .writeStream\\\n",
        "    .outputMode('update')\\\n",
        "    .foreachBatch(foreach_batch_function)\\\n",
        "    .start()\n",
        "query.awaitTermination(40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-H5AO3E-L3Ne"
      },
      "outputs": [],
      "source": [
        "# Stop job\n",
        "query.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVw13YK6L3Ne"
      },
      "source": [
        "## Resposta Atividade 2\n",
        "\n",
        "Quais são os top 3 anúncios mais clicados dos últimos 20 segundos? Calcule a cada 15 segundos\n",
        "\n",
        "**Resposta:**  \n",
        "```\n",
        "                                        window     adId  count\n",
        "0   (2023-09-01 02:41:45, 2023-09-01 02:42:05)  adId_06   3400\n",
        "1   (2023-09-01 02:41:45, 2023-09-01 02:42:05)  adId_07   3342\n",
        "2   (2023-09-01 02:41:45, 2023-09-01 02:42:05)  adId_02   3225\n",
        "3   (2023-09-01 02:42:00, 2023-09-01 02:42:20)  adId_06  12955\n",
        "4   (2023-09-01 02:42:00, 2023-09-01 02:42:20)  adId_07  12419\n",
        "5   (2023-09-01 02:42:00, 2023-09-01 02:42:20)  adId_02  12198\n",
        "6   (2023-09-01 02:42:15, 2023-09-01 02:42:35)  adId_06  10203\n",
        "7   (2023-09-01 02:42:15, 2023-09-01 02:42:35)  adId_08   9480\n",
        "8   (2023-09-01 02:42:15, 2023-09-01 02:42:35)  adId_07   9402\n",
        "9   (2023-09-01 02:42:30, 2023-09-01 02:42:50)  adId_06    662\n",
        "10  (2023-09-01 02:42:30, 2023-09-01 02:42:50)  adId_08    644\n",
        "11  (2023-09-01 02:42:30, 2023-09-01 02:42:50)  adId_03    643\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A função `foreach_batch_function` é registrada como um manipulador para cada lote de dados que chega ao final de um microbatch durante o processo de streaming. Isso permite executar operações de processamento personalizadas em cada lote de dados à medida que são processados pelo Spark Streaming.\n",
        "\n",
        "1. Argumentos da Função:\n",
        "\n",
        "> * df: Este é o DataFrame que contém os dados do lote atual (batch). É um DataFrame do Spark que representa os dados que chegaram durante o intervalo de tempo especificado.\n",
        "\n",
        "\n",
        "> * epoch_id: Este é o identificador único do lote atual. Cada vez que a função é chamada para processar um lote, ela recebe um novo epoch_id.\n",
        "\n",
        "2. Processamento do DataFrame:\n",
        "\n",
        "> * Dentro da função, o DataFrame df é processado para encontrar os três principais anúncios (adId) em cada janela de tempo.\n",
        "> * Um Window é criado para particionar os dados por window.start e ordená-los pela contagem decrescente de cliques (count). Isso é feito para garantir que os três principais anúncios sejam selecionados corretamente dentro de cada janela de tempo.\n",
        "> * A função row_number() é usada para atribuir um número de linha a cada registro dentro de cada janela de tempo, com base na ordenação definida na janela.\n",
        "> * Em seguida, o DataFrame é filtrado para manter apenas os três principais registros (anúncios) dentro de cada janela de tempo, utilizando a coluna de classificação atribuída anteriormente.\n",
        "> * Finalmente, o DataFrame é ordenado novamente pela janela de tempo para garantir que os resultados sejam exibidos em ordem cronológica.\n"
      ],
      "metadata": {
        "id": "g_SYmxkx6w8E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrZyx9kOL3Ne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12e6affd-1c1b-4816-f780-1e83a9039ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "                                        window     adId  count\n",
            "0   (2023-09-01 02:41:45, 2023-09-01 02:42:05)  adId_06   3400\n",
            "1   (2023-09-01 02:41:45, 2023-09-01 02:42:05)  adId_07   3342\n",
            "2   (2023-09-01 02:41:45, 2023-09-01 02:42:05)  adId_02   3225\n",
            "3   (2023-09-01 02:42:00, 2023-09-01 02:42:20)  adId_06  12955\n",
            "4   (2023-09-01 02:42:00, 2023-09-01 02:42:20)  adId_07  12419\n",
            "5   (2023-09-01 02:42:00, 2023-09-01 02:42:20)  adId_02  12198\n",
            "6   (2023-09-01 02:42:15, 2023-09-01 02:42:35)  adId_06  10203\n",
            "7   (2023-09-01 02:42:15, 2023-09-01 02:42:35)  adId_08   9480\n",
            "8   (2023-09-01 02:42:15, 2023-09-01 02:42:35)  adId_07   9402\n",
            "9   (2023-09-01 02:42:30, 2023-09-01 02:42:50)  adId_06    662\n",
            "10  (2023-09-01 02:42:30, 2023-09-01 02:42:50)  adId_08    644\n",
            "11  (2023-09-01 02:42:30, 2023-09-01 02:42:50)  adId_03    643\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "inputStream = spark.readStream.csv(\n",
        "    AD_ACTION_CSV_PATH,\n",
        "    schema=\"timestamp TIMESTAMP, \\\n",
        "        user_id STRING, \\\n",
        "        action STRING, \\\n",
        "        adId STRING, \\\n",
        "        campaignId STRING\"\n",
        ")\n",
        "\n",
        "# ESCREVA SEU CÓDIGO AQUI\n",
        "inputStream = inputStream\\\n",
        "    .where(F.col('action')=='click')\\\n",
        "    .groupBy(\n",
        "        F.window('timestamp', '20 seconds', '15 seconds'),\n",
        "        'adId'\n",
        "    ).count()\n",
        "\n",
        "\n",
        "def foreach_batch_function(df, epoch_id):\n",
        "    window_group = Window.partitionBy('window.start')\\\n",
        "        .orderBy(F.desc('count'))\n",
        "    df = df.withColumn('rank', F.row_number().over(window_group))\\\n",
        "        .where(F.col('rank') <= 3)\\\n",
        "        .drop('rank')\\\n",
        "        .orderBy(F.asc('window.start'))\n",
        "    print(epoch_id)\n",
        "    print(df.toPandas())\n",
        "\n",
        "query = inputStream\\\n",
        "    .writeStream\\\n",
        "    .outputMode('complete')\\\n",
        "    .foreachBatch(foreach_batch_function)\\\n",
        "    .start()\n",
        "query.awaitTermination(40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxGaT4E7L3Ne"
      },
      "outputs": [],
      "source": [
        "# Stop job\n",
        "query.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWQDvr_1L3Ne"
      },
      "source": [
        "## Resposta Atividade 3\n",
        "\n",
        "Qual é o usuário que tem mais eventos dos últimos 15 segundos? Calcule a cada 5 segundos.\n",
        "\n",
        "**Resposta:**  \n",
        "```\n",
        "                                       window                               user_id  count\n",
        "0  (2023-09-01 02:41:50, 2023-09-01 02:42:05)  c6ae5c85-c423-49f0-8e26-de11d4eea06a     23\n",
        "1  (2023-09-01 02:41:55, 2023-09-01 02:42:10)  c6ae5c85-c423-49f0-8e26-de11d4eea06a     43\n",
        "2  (2023-09-01 02:42:00, 2023-09-01 02:42:15)  c6ae5c85-c423-49f0-8e26-de11d4eea06a     57\n",
        "3  (2023-09-01 02:42:05, 2023-09-01 02:42:20)  294d49e9-793a-4de7-b36e-9f07df17896a     41\n",
        "4  (2023-09-01 02:42:10, 2023-09-01 02:42:25)  49822c4b-1de3-499d-ad72-c13ad9c1b35d     35\n",
        "5  (2023-09-01 02:42:15, 2023-09-01 02:42:30)  ae6ff16a-a693-4894-96ae-5c514e35a334     54\n",
        "6  (2023-09-01 02:42:20, 2023-09-01 02:42:35)  ae6ff16a-a693-4894-96ae-5c514e35a334     50\n",
        "7  (2023-09-01 02:42:25, 2023-09-01 02:42:40)  0e965fef-50f7-4036-a258-2dfad8fa7648     31\n",
        "8  (2023-09-01 02:42:30, 2023-09-01 02:42:45)  766ecd74-f9cd-4e1d-b5ea-05baafe52468      8\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3d8spytL3Nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "303d7e86-5db4-4895-9b0e-6d040ad15848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       window  \\\n",
            "0  (2023-09-01 02:41:50, 2023-09-01 02:42:05)   \n",
            "1  (2023-09-01 02:41:55, 2023-09-01 02:42:10)   \n",
            "2  (2023-09-01 02:42:00, 2023-09-01 02:42:15)   \n",
            "3  (2023-09-01 02:42:05, 2023-09-01 02:42:20)   \n",
            "4  (2023-09-01 02:42:10, 2023-09-01 02:42:25)   \n",
            "5  (2023-09-01 02:42:15, 2023-09-01 02:42:30)   \n",
            "6  (2023-09-01 02:42:20, 2023-09-01 02:42:35)   \n",
            "7  (2023-09-01 02:42:25, 2023-09-01 02:42:40)   \n",
            "8  (2023-09-01 02:42:30, 2023-09-01 02:42:45)   \n",
            "\n",
            "                                user_id  count  \n",
            "0  c6ae5c85-c423-49f0-8e26-de11d4eea06a     23  \n",
            "1  c6ae5c85-c423-49f0-8e26-de11d4eea06a     43  \n",
            "2  c6ae5c85-c423-49f0-8e26-de11d4eea06a     57  \n",
            "3  294d49e9-793a-4de7-b36e-9f07df17896a     41  \n",
            "4  49822c4b-1de3-499d-ad72-c13ad9c1b35d     35  \n",
            "5  ae6ff16a-a693-4894-96ae-5c514e35a334     54  \n",
            "6  ae6ff16a-a693-4894-96ae-5c514e35a334     50  \n",
            "7  0e965fef-50f7-4036-a258-2dfad8fa7648     31  \n",
            "8  766ecd74-f9cd-4e1d-b5ea-05baafe52468      8  \n"
          ]
        }
      ],
      "source": [
        "inputStream = spark.readStream.csv(\n",
        "    AD_ACTION_CSV_PATH,\n",
        "    schema=\"timestamp TIMESTAMP, \\\n",
        "        user_id STRING, \\\n",
        "        action STRING, \\\n",
        "        adId STRING, \\\n",
        "        campaignId STRING\"\n",
        ")\n",
        "\n",
        "\n",
        "inputStream = inputStream\\\n",
        "    .groupBy(\n",
        "        F.window('timestamp', '15 seconds', '5 seconds'),\n",
        "        'user_id'\n",
        "    ).count()\n",
        "\n",
        "\n",
        "def foreach_batch_function(df, epoch_id):\n",
        "    window_group = Window.partitionBy('window.start')\\\n",
        "        .orderBy(F.desc('count'))\n",
        "    df = df.withColumn('rank', F.row_number().over(window_group))\\\n",
        "        .where(F.col('rank') <= 1)\\\n",
        "        .drop('rank')\\\n",
        "        .orderBy(F.asc('window.start'))\n",
        "    print(epoch_id)\n",
        "    print(df.toPandas())\n",
        "\n",
        "query = inputStream\\\n",
        "    .writeStream\\\n",
        "    .outputMode('complete')\\\n",
        "    .foreachBatch(foreach_batch_function)\\\n",
        "    .start()\n",
        "query.awaitTermination(40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdq82EE1L3Nf"
      },
      "outputs": [],
      "source": [
        "# Stop job\n",
        "query.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH9SZGgPL3Nf"
      },
      "source": [
        "## Resposta Atividade 4\n",
        "\n",
        "Qual campanha teve a maior diminuição de eventos em relação ao segundo anterior? Calcule de segundo em segundo.\n",
        "\n",
        "**Resposta:**\n",
        "```\n",
        "                                        window     campaignId   diff\n",
        "0   (2023-09-01 02:42:00, 2023-09-01 02:42:01)  campaignId_01   17.0\n",
        "1   (2023-09-01 02:42:01, 2023-09-01 02:42:02)  campaignId_01   33.0\n",
        "2   (2023-09-01 02:42:02, 2023-09-01 02:42:03)  campaignId_01   41.0\n",
        "3   (2023-09-01 02:42:03, 2023-09-01 02:42:04)  campaignId_01   62.0\n",
        "4   (2023-09-01 02:42:04, 2023-09-01 02:42:05)  campaignId_03  105.0\n",
        "5   (2023-09-01 02:42:05, 2023-09-01 02:42:06)  campaignId_01   85.0\n",
        "6   (2023-09-01 02:42:06, 2023-09-01 02:42:07)  campaignId_03   63.0\n",
        "7   (2023-09-01 02:42:07, 2023-09-01 02:42:08)  campaignId_02  139.0\n",
        "8   (2023-09-01 02:42:08, 2023-09-01 02:42:09)  campaignId_03  119.0\n",
        "9   (2023-09-01 02:42:09, 2023-09-01 02:42:10)  campaignId_01   14.0\n",
        "10  (2023-09-01 02:42:10, 2023-09-01 02:42:11)  campaignId_02   79.0\n",
        "11  (2023-09-01 02:42:11, 2023-09-01 02:42:12)  campaignId_03   92.0\n",
        "12  (2023-09-01 02:42:12, 2023-09-01 02:42:13)  campaignId_01   92.0\n",
        "13  (2023-09-01 02:42:13, 2023-09-01 02:42:14)  campaignId_03   74.0\n",
        "14  (2023-09-01 02:42:14, 2023-09-01 02:42:15)  campaignId_01   57.0\n",
        "15  (2023-09-01 02:42:16, 2023-09-01 02:42:17)  campaignId_01  111.0\n",
        "16  (2023-09-01 02:42:17, 2023-09-01 02:42:18)  campaignId_02  103.0\n",
        "17  (2023-09-01 02:42:18, 2023-09-01 02:42:19)  campaignId_01   39.0\n",
        "18  (2023-09-01 02:42:19, 2023-09-01 02:42:20)  campaignId_03   60.0\n",
        "19  (2023-09-01 02:42:20, 2023-09-01 02:42:21)  campaignId_02   32.0\n",
        "20  (2023-09-01 02:42:21, 2023-09-01 02:42:22)  campaignId_02  148.0\n",
        "21  (2023-09-01 02:42:22, 2023-09-01 02:42:23)  campaignId_03   79.0\n",
        "22  (2023-09-01 02:42:23, 2023-09-01 02:42:24)  campaignId_03   51.0\n",
        "23  (2023-09-01 02:42:24, 2023-09-01 02:42:25)  campaignId_01   24.0\n",
        "24  (2023-09-01 02:42:25, 2023-09-01 02:42:26)  campaignId_02   81.0\n",
        "25  (2023-09-01 02:42:26, 2023-09-01 02:42:27)  campaignId_03  129.0\n",
        "26  (2023-09-01 02:42:27, 2023-09-01 02:42:28)  campaignId_01  103.0\n",
        "27  (2023-09-01 02:42:28, 2023-09-01 02:42:29)  campaignId_03  209.0\n",
        "28  (2023-09-01 02:42:29, 2023-09-01 02:42:30)  campaignId_02   97.0\n",
        "29  (2023-09-01 02:42:30, 2023-09-01 02:42:31)  campaignId_02    NaN\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHvU_iprL3Nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03045c29-a90e-4bde-ee99-b657efed7981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "                                        window     campaignId  count    lead   diff  row\n",
            "0   (2023-09-01 02:42:00, 2023-09-01 02:42:01)  campaignId_01   2643  2626.0   17.0    1\n",
            "1   (2023-09-01 02:42:01, 2023-09-01 02:42:02)  campaignId_01   2626  2593.0   33.0    1\n",
            "2   (2023-09-01 02:42:02, 2023-09-01 02:42:03)  campaignId_01   2593  2552.0   41.0    1\n",
            "3   (2023-09-01 02:42:03, 2023-09-01 02:42:04)  campaignId_01   2552  2490.0   62.0    1\n",
            "4   (2023-09-01 02:42:04, 2023-09-01 02:42:05)  campaignId_03   2898  2793.0  105.0    1\n",
            "5   (2023-09-01 02:42:05, 2023-09-01 02:42:06)  campaignId_01   2623  2538.0   85.0    1\n",
            "6   (2023-09-01 02:42:06, 2023-09-01 02:42:07)  campaignId_03   2905  2842.0   63.0    1\n",
            "7   (2023-09-01 02:42:07, 2023-09-01 02:42:08)  campaignId_02   3065  2926.0  139.0    1\n",
            "8   (2023-09-01 02:42:08, 2023-09-01 02:42:09)  campaignId_03   2938  2819.0  119.0    1\n",
            "9   (2023-09-01 02:42:09, 2023-09-01 02:42:10)  campaignId_01   2591  2577.0   14.0    1\n",
            "10  (2023-09-01 02:42:10, 2023-09-01 02:42:11)  campaignId_02   3102  3023.0   79.0    1\n",
            "11  (2023-09-01 02:42:11, 2023-09-01 02:42:12)  campaignId_03   2886  2794.0   92.0    1\n",
            "12  (2023-09-01 02:42:12, 2023-09-01 02:42:13)  campaignId_01   2676  2584.0   92.0    1\n",
            "13  (2023-09-01 02:42:13, 2023-09-01 02:42:14)  campaignId_03   2905  2831.0   74.0    1\n",
            "14  (2023-09-01 02:42:14, 2023-09-01 02:42:15)  campaignId_01   2671  2614.0   57.0    1\n",
            "15  (2023-09-01 02:42:16, 2023-09-01 02:42:17)  campaignId_01   2614  2503.0  111.0    1\n",
            "16  (2023-09-01 02:42:17, 2023-09-01 02:42:18)  campaignId_02   3088  2985.0  103.0    1\n",
            "17  (2023-09-01 02:42:18, 2023-09-01 02:42:19)  campaignId_01   2487  2448.0   39.0    1\n",
            "18  (2023-09-01 02:42:19, 2023-09-01 02:42:20)  campaignId_03   3004  2944.0   60.0    1\n",
            "19  (2023-09-01 02:42:20, 2023-09-01 02:42:21)  campaignId_02   3123  3091.0   32.0    1\n",
            "20  (2023-09-01 02:42:21, 2023-09-01 02:42:22)  campaignId_02   3091  2943.0  148.0    1\n",
            "21  (2023-09-01 02:42:22, 2023-09-01 02:42:23)  campaignId_03   3046  2967.0   79.0    1\n",
            "22  (2023-09-01 02:42:23, 2023-09-01 02:42:24)  campaignId_03   2967  2916.0   51.0    1\n",
            "23  (2023-09-01 02:42:24, 2023-09-01 02:42:25)  campaignId_01   2511  2487.0   24.0    1\n",
            "24  (2023-09-01 02:42:25, 2023-09-01 02:42:26)  campaignId_02   3080  2999.0   81.0    1\n",
            "25  (2023-09-01 02:42:26, 2023-09-01 02:42:27)  campaignId_03   2991  2862.0  129.0    1\n",
            "26  (2023-09-01 02:42:27, 2023-09-01 02:42:28)  campaignId_01   2546  2443.0  103.0    1\n",
            "27  (2023-09-01 02:42:28, 2023-09-01 02:42:29)  campaignId_03   3050  2841.0  209.0    1\n",
            "28  (2023-09-01 02:42:29, 2023-09-01 02:42:30)  campaignId_02   3106  3009.0   97.0    1\n",
            "29  (2023-09-01 02:42:30, 2023-09-01 02:42:31)  campaignId_03   2894     NaN    NaN    1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "from pyspark.sql.functions import lead\n",
        "\n",
        "inputStream = spark.readStream.csv(\n",
        "    AD_ACTION_CSV_PATH,\n",
        "    schema=\"timestamp TIMESTAMP, \\\n",
        "        user_id STRING, \\\n",
        "        action STRING, \\\n",
        "        adId STRING, \\\n",
        "        campaignId STRING\"\n",
        ")\n",
        "\n",
        "inputStream = inputStream\\\n",
        "    .groupBy(\n",
        "        F.window('timestamp', '1 seconds', '1 seconds'), 'campaignId'\n",
        "    ).count().orderBy('window') # O DataFrame resultante é ordenado pela coluna de janela de tempo.\n",
        "\n",
        "def foreach_batch_function(df, epoch_id):\n",
        "    windowSpec  = Window.partitionBy(\"campaignId\").orderBy(\"window.start\")\n",
        "\n",
        "    df = df.withColumn(\"lead\", lead(\"count\", 1).over(windowSpec))\n",
        "    df = df.withColumn(\"diff\", F.col('count') - F.col('lead'))\n",
        "\n",
        "    windowDept = Window.partitionBy(\"window.start\").orderBy(F.col(\"diff\").desc())\n",
        "    df = df.withColumn(\"row\", F.row_number().over(windowDept)) \\\n",
        "    .filter(F.col(\"row\") <= 1) \\\n",
        "    .orderBy(F.asc('window.start'))\n",
        "\n",
        "    print(epoch_id)\n",
        "    print(df.toPandas())\n",
        "\n",
        "query = inputStream\\\n",
        "    .writeStream\\\n",
        "    .outputMode('complete')\\\n",
        "    .foreachBatch(foreach_batch_function)\\\n",
        "    .start()\n",
        "query.awaitTermination(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpmjJI0fL3Nf"
      },
      "outputs": [],
      "source": [
        "# Stop job\n",
        "query.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRk1q5NzL3Ng"
      },
      "source": [
        "## Resposta Atividade 5\n",
        "\n",
        "Qual é a porcentagem de clicks dado o total de eventos dos últimos 12 segundos? Calcule a cada 6 segundos.\n",
        "\n",
        "**Resposta**  \n",
        "```\n",
        "                                       window  percentage\n",
        "0  (2023-09-01 02:41:54, 2023-09-01 02:42:06)       69.84\n",
        "1  (2023-09-01 02:42:00, 2023-09-01 02:42:12)       69.84\n",
        "2  (2023-09-01 02:42:06, 2023-09-01 02:42:18)       70.00\n",
        "3  (2023-09-01 02:42:12, 2023-09-01 02:42:24)       69.91\n",
        "4  (2023-09-01 02:42:18, 2023-09-01 02:42:30)       70.08\n",
        "5  (2023-09-01 02:42:24, 2023-09-01 02:42:36)       70.42\n",
        "6  (2023-09-01 02:42:30, 2023-09-01 02:42:42)       70.04\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Função `foreach_batch_function()`:\n",
        "\n",
        "\n",
        "1. `df = df.withColumn('total_events', F.col('count')+F.lag(F.col('count'), 1).over(windowSpec))`:\n",
        "- Nesta linha, estamos calculando o total de eventos para cada janela de tempo (window). Isso é feito somando a contagem atual de eventos (count) com o valor da contagem de eventos na linha anterior dentro da mesma janela de tempo. Para isso, usamos a função lag do Spark para acessar o valor anterior da contagem de eventos dentro da janela. O resultado é armazenado na coluna total_events.\n",
        "\n",
        "2. `df = df.withColumn(\"total_events\", F.last('total_events', True).over(windowSpec))`:\n",
        "- Aqui, estamos preenchendo os valores null na coluna total_events com o último valor não nulo encontrado dentro da mesma janela de tempo. Isso é feito usando a função last do Spark com o argumento True, que especifica para ignorar os valores null. O resultado é que todas as linhas dentro da mesma janela de tempo terão o mesmo valor para total_events, que representa o total de eventos dentro dessa janela."
      ],
      "metadata": {
        "id": "KcxmakyhXCgo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpHwKMOML3Ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67446632-0bdf-4f8f-d97c-3d73228decd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "                                       window action  count  total_events  percent\n",
            "0  (2023-09-01 02:41:54, 2023-09-01 02:42:06)  click  35580         50946    69.84\n",
            "1  (2023-09-01 02:42:00, 2023-09-01 02:42:12)  click  71165        101892    69.84\n",
            "2  (2023-09-01 02:42:06, 2023-09-01 02:42:18)  click  65377         93397    70.00\n",
            "3  (2023-09-01 02:42:12, 2023-09-01 02:42:24)  click  65289         93391    69.91\n",
            "4  (2023-09-01 02:42:18, 2023-09-01 02:42:30)  click  71402        101880    70.08\n",
            "5  (2023-09-01 02:42:24, 2023-09-01 02:42:36)  click  41851         59430    70.42\n",
            "6  (2023-09-01 02:42:30, 2023-09-01 02:42:42)  click   5946          8490    70.04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ],
      "source": [
        "inputStream = spark.readStream.csv(\n",
        "    AD_ACTION_CSV_PATH,\n",
        "    schema=\"timestamp TIMESTAMP, \\\n",
        "        user_id STRING, \\\n",
        "        action STRING, \\\n",
        "        adId STRING, \\\n",
        "        campaignId STRING\"\n",
        ")\n",
        "\n",
        "inputStream = inputStream\\\n",
        "    .groupBy(\n",
        "        F.window('timestamp', '12 seconds', '6 seconds'),\n",
        "        'action'\n",
        "    ).count()\n",
        "\n",
        "def foreach_batch_function(df, epoch_id):\n",
        "    windowSpec  = Window.partitionBy(\"window\").orderBy(\"window.start\")\n",
        "\n",
        "    df = df.withColumn('total_events', F.col('count')+F.lag(F.col('count'), 1).over(windowSpec))\n",
        "    df = df.withColumn(\"total_events\", F.last('total_events', True).over(windowSpec))\n",
        "    df = df.where(F.col('action') == 'click')\n",
        "    df = df.withColumn('percent', F.round(100*(F.col('count')/F.col('total_events')), 2))\n",
        "    df = df.orderBy(F.asc('window.start'))\n",
        "\n",
        "    print(epoch_id)\n",
        "    print(df.toPandas())\n",
        "\n",
        "query = inputStream\\\n",
        "    .writeStream\\\n",
        "    .outputMode('complete')\\\n",
        "    .foreachBatch(foreach_batch_function)\\\n",
        "    .start()\n",
        "query.awaitTermination(40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve7wgWdmL3Ng"
      },
      "outputs": [],
      "source": [
        "# Stop job\n",
        "query.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}